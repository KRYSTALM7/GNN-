{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889cdfa3",
   "metadata": {},
   "source": [
    "# ISOLATION FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b358e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Step 3: Handle missing values\u001b[39;00m\n\u001b[0;32m     15\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m), columns\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     17\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Import the data\n",
    "data = pd.read_csv(r'S:\\CODING NOTES\\PYTHON3.0\\CSV files\\preprocessed Sensor_data.csv')\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "features = data.drop(['machine_status', 'date'], axis=1)  # Exclude label and date columns\n",
    "labels = data['machine_status']\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the anomaly detection model\n",
    "model = IsolationForest(contamination=0.05)  # Adjust contamination parameter as needed\n",
    "model.fit(X_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "train_scores = model.decision_function(X_train)\n",
    "test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Step 5: Visualize the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_scores, bins=50, label='Normal')\n",
    "plt.hist(test_scores, bins=50, label='Anomaly')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f3b3ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 7: Correct any misspelled labels\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43my_train\u001b[49m[y_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNORMA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNORMAL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m y_test[y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNORMA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNORMAL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Step 8: Encode labels\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Step 7: Correct any misspelled labels\n",
    "y_train[y_train == 'NORMA'] = 'NORMAL'\n",
    "y_test[y_test == 'NORMA'] = 'NORMAL'\n",
    "\n",
    "# Step 8: Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")  # Ignore the UndefinedMetricWarning\n",
    "    train_cm = confusion_matrix(y_train_encoded, train_predictions, labels=[0, 1])\n",
    "    test_cm = confusion_matrix(y_test_encoded, test_predictions, labels=[0, 1])\n",
    "\n",
    "train_report = classification_report(y_train_encoded, train_predictions, labels=[0, 1])\n",
    "test_report = classification_report(y_test_encoded, test_predictions, labels=[0, 1])\n",
    "\n",
    "print(\"Train Classification Report:\\n\", train_report)\n",
    "print(\"Test Classification Report:\\n\", test_report)\n",
    "\n",
    "# Step 9: Visualize confusion matrix\n",
    "labels = label_encoder.classes_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Train Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad82dce",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2303703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=220320, num_edges=220319,\n",
      "      ndata_schemes={'features': Scheme(shape=(51,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUJAN\\anaconda3\\envs\\myenv\\lib\\site-packages\\dgl\\heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Step 1: Read the preprocessed data\n",
    "data = pd.read_csv(r'S:\\CODING NOTES\\PYTHON3.0\\CSV files\\preprocessed Sensor_data.csv')\n",
    "\n",
    "# Step 2: Extract sensor readings and machine status\n",
    "sensor_data = data.iloc[:, :-2].values\n",
    "machine_status = data['machine_status'].values\n",
    "\n",
    "# Step 3: Create graph and add nodes\n",
    "num_nodes = sensor_data.shape[0]\n",
    "graph = dgl.DGLGraph()\n",
    "graph.add_nodes(num_nodes)\n",
    "\n",
    "# Step 4: Add node features (sensor readings)\n",
    "graph.ndata['features'] = torch.tensor(sensor_data, dtype=torch.float32)\n",
    "\n",
    "# Step 5: Create edges based on time intervals\n",
    "# Assuming each row represents a time interval, create edges between consecutive time intervals\n",
    "src = np.arange(num_nodes - 1)\n",
    "dst = np.arange(1, num_nodes)\n",
    "graph.add_edges(src, dst)\n",
    "\n",
    "# Step 6: Set machine status as node labels\n",
    "machine_status = np.where(machine_status == 'NORMAL', 0, 1)  # Convert labels to numeric type\n",
    "graph.ndata['label'] = torch.tensor(machine_status, dtype=torch.long)\n",
    "\n",
    "# Print the graph information\n",
    "print(graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058bca84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 7: Split the graph into training and testing sets\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mtrain_mask\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m test_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(test_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m train_graph \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39msubgraph(train_mask)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_mask' is not defined"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 7: Split the graph into training and testing sets\n",
    "train_mask = np.where(train_mask)[0]\n",
    "test_mask = np.where(test_mask)[0]\n",
    "\n",
    "train_graph = graph.subgraph(train_mask)\n",
    "test_graph = graph.subgraph(test_mask)\n",
    "\n",
    "# Step 7.1: Add self-loops to the graph\n",
    "graph = dgl.add_self_loop(graph)\n",
    "\n",
    "# Step 8: Define and train the GNN model\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = torch.relu(self.conv1(g, features))\n",
    "        x = self.conv2(g, x)\n",
    "        return x\n",
    "\n",
    "in_feats = sensor_data.shape[1]  # Number of input features\n",
    "hidden_size = 64\n",
    "num_classes = 2  # Normal and anomaly\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4528e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "in_feats = sensor_data.shape[1]  # Number of input features\n",
    "hidden_size = 64\n",
    "num_classes = 2  # Normal and anomaly\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "print(in_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(in_feats, hidden_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, graph, features, labels, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(graph, features)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, graph, features, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        accuracy = (predicted == labels).float().mean().item()\n",
    "        report = classification_report(labels, predicted, target_names=['NORMAL', 'ANOMALY'])\n",
    "        cm = confusion_matrix(labels, predicted)\n",
    "    return accuracy, report, cm\n",
    "\n",
    "train_features = train_graph.ndata['features']\n",
    "train_labels = train_graph.ndata['label']\n",
    "test_features = test_graph.ndata['features']\n",
    "test_labels = test_graph.ndata['label']\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_graph, train_features, train_labels, optimizer, criterion)\n",
    "    test_accuracy, test_report, test_cm = evaluate(model, test_graph, test_features, test_labels)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_accuracy)\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Step 9: Visualize confusion matrix\n",
    "labels = ['NORMAL', 'ANOMALY']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Train Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Test Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e56c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")  # Ignore the UndefinedMetricWarning\n",
    "    train_cm = confusion_matrix(y_train_encoded, train_predictions, labels=[0, 1])\n",
    "    test_cm = confusion_matrix(y_test_encoded, test_predictions, labels=[0, 1])\n",
    "\n",
    "train_report = classification_report(y_train_encoded, train_predictions, labels=[0, 1])\n",
    "test_report = classification_report(y_test_encoded, test_predictions, labels=[0, 1])\n",
    "\n",
    "print(\"Train Classification Report:\\n\", train_report)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
